{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used car auction prices dataset assignment\n",
    "\n",
    "#### Research Question: \n",
    "    As a car salesman, how can you effectively predict car sales prices based on features such as color, year, odometer reading, and condition, thereby facilitating our business in making informed pricing decisions?\n",
    "\n",
    "#### Sub Questions: \n",
    "    1. How do the features impact the accuracy of predictions from the Random Forest, ANN, KNN, XGBoost, and Linear Regression models?\n",
    "    2. What hyperparameter configurations for each model (Random Forest, ANN, KNN, XGBoost) result in the best predictive accuracy for car sales prices?\n",
    "    3. How do the Random Forest, ANN, KNN, XGBoost, and Linear Regression models demonstrate robustness in predicting car sales prices?\n",
    "    4. How do the Random Forest, ANN, KNN, XGBoost, and Linear Regression models compare in terms of their predictive performance for car sales prices, and can we identify specific scenarios or feature patterns where one model consistently outperforms the others?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import * \n",
    "import os\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "from IPython.core.display import HTML \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read data from path\n",
    "\n",
    "An extra column is loaded due to having data but no column name. \n",
    "This is because of an error of data in the dataset and will be adjusted after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# List all the directories and files in the parent directory of 'dataset'\n",
    "dir_list = os.listdir(os.path.join(cwd, ''))\n",
    "\n",
    "car_prices_data_path = os.path.join( \"\", \"car_prices.csv\")\n",
    "\n",
    "# Read csv file and name column to make sure the 17th column is included\n",
    "df = pd.read_csv(car_prices_data_path, header=None, names=[f'col{i}' for i in range(1, 18)])\n",
    "\n",
    "# Give the first row of this column a value\n",
    "df.iloc[0, 16] = 'extra'\n",
    "\n",
    "# Now, use the first row as column names\n",
    "df.columns = df.iloc[0]\n",
    "\n",
    "# Drop the first row since it's now redundant as column names\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Examine and Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unalligned data is alligned into the correct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with data in the 'extra' column\n",
    "df_extra = df[df['extra'].notnull()]\n",
    "\n",
    "# Include columns starting from column 5 onwards\n",
    "df_extra = df_extra.iloc[:, 5:]\n",
    "\n",
    "# Replace column names with the name of the column to the right\n",
    "df_extra.columns = df_extra.columns.to_series().shift(+1).fillna('body')\n",
    "\n",
    "# Iterate over the rows of df_extra and update values in df\n",
    "for index, row in df_extra.iterrows():\n",
    "    for column_name in df.columns:\n",
    "        # Update values in df only for shared rows and columns\n",
    "        if column_name in df_extra.columns and index in df.index:\n",
    "            df.at[index, column_name] = row[column_name]\n",
    "\n",
    "# drop the 'extra' column from the dataframe\n",
    "df = df.drop('extra', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to appropriate data types\n",
    "df['year'] = df['year'].astype('int')\n",
    "df['odometer'] = df['odometer'].astype('float')\n",
    "df['mmr'] = df['mmr'].astype('float')\n",
    "df['sellingprice'] = df['sellingprice'].astype('float')\n",
    "df['condition'] = df['condition'].astype('float')\n",
    "df['saledate'] = pd.to_datetime(df['saledate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate shape\n",
    "print(\"There are {} rows and {} columns in the dataset\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "# get statistics for numeric columns\n",
    "nan_counts_numeric = numeric_columns.isnull().sum()\n",
    "unique_counts_numeric = numeric_columns.nunique()\n",
    "mode_counts_numeric = numeric_columns.mode().iloc[0]\n",
    "max_values_numeric = numeric_columns.max()\n",
    "min_values_numeric = numeric_columns.min()\n",
    "percentage_nan_numeric = numeric_columns.isnull().mean() * 100\n",
    "mean_values_numeric = numeric_columns.mean()\n",
    "median_values_numeric = numeric_columns.median()\n",
    "variance_values_numeric = numeric_columns.var()\n",
    "std_dev_values_numeric = numeric_columns.std()\n",
    "skewness_values_numeric = numeric_columns.skew()\n",
    "kurtosis_values_numeric = numeric_columns.kurt()\n",
    "\n",
    "# create a new dataframe with the statistics for numeric columns\n",
    "numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_numeric,\n",
    "                                    'Unique Count': unique_counts_numeric,\n",
    "                                    'Mode Count': mode_counts_numeric,\n",
    "                                    'Max Value': max_values_numeric,\n",
    "                                    'Min Value': min_values_numeric,\n",
    "                                    'Percentage Nan': percentage_nan_numeric,\n",
    "                                    'Mean Value': mean_values_numeric,\n",
    "                                    'Median Value': median_values_numeric,\n",
    "                                    'Variance Value': variance_values_numeric,\n",
    "                                    'Standard Deviation Value': std_dev_values_numeric,\n",
    "                                    'Skewness Value': skewness_values_numeric,\n",
    "                                    'Kurtosis Value': kurtosis_values_numeric})\n",
    "\n",
    "# Print the numeric statistics dataframe\n",
    "print(tabulate(numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non numeric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=['number'])\n",
    "\n",
    "# get statistics for non-numeric columns\n",
    "unique_counts_non_numeric = non_numeric_columns.nunique()\n",
    "mode_counts_non_numeric = non_numeric_columns.mode().iloc[0]\n",
    "nan_counts_non_numeric = non_numeric_columns.isnull().sum()\n",
    "percentage_nan_non_numeric = non_numeric_columns.isnull().mean() * 100\n",
    "\n",
    "# create a new dataframe with the statistics for non-numeric columns\n",
    "non_numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_non_numeric,\n",
    "                                       'Unique Count': unique_counts_non_numeric,\n",
    "                                       'Mode Count': mode_counts_non_numeric,\n",
    "                                       'Percentage Nan': percentage_nan_non_numeric})\n",
    "\n",
    "# print the non-numeric statistics dataframe\n",
    "print(tabulate(non_numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('model')\n",
    "modes = grouped['transmission'].apply(lambda x: stats.mode(x)[0][0])\n",
    "df['transmission'].fillna(df['model'].map(modes), inplace=True)\n",
    "\n",
    "# Calculate the mode of the 'transmission' column\n",
    "mode = df['transmission'].mode()[0]\n",
    "\n",
    "# Fill the remaining NaN values in the 'transmission' column with the calculated mode\n",
    "df['transmission'].fillna(mode, inplace=True)\n",
    "\n",
    "# Calculate the mode of the 'transmission' column\n",
    "mode = df['transmission'].mode()[0]\n",
    "\n",
    "# Drop all rows with NaN values in the other rows\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if NaN values are filtered out and shape of remaining dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(exclude=['number'])\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "# get statistics for numeric columns\n",
    "nan_counts_numeric = numeric_columns.isnull().sum()\n",
    "unique_counts_numeric = numeric_columns.nunique()\n",
    "percentage_nan_numeric = numeric_columns.isnull().mean() * 100\n",
    "\n",
    "\n",
    "# create a new dataframe with the statistics for numeric columns\n",
    "numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_numeric,\n",
    "                                    'Unique Count': unique_counts_numeric,\n",
    "                                    'Percentage Nan': percentage_nan_numeric})\n",
    "\n",
    "\n",
    "# print the numeric statistics dataframe\n",
    "print(tabulate(numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "# get statistics for non-numeric columns\n",
    "unique_counts_non_numeric = non_numeric_columns.nunique()\n",
    "nan_counts_non_numeric = non_numeric_columns.isnull().sum()\n",
    "percentage_nan_non_numeric = non_numeric_columns.isnull().mean() * 100\n",
    "\n",
    "# create a new dataframe with the statistics for non-numeric columns\n",
    "non_numeric_statistics = pd.DataFrame({'Nan Count': nan_counts_non_numeric,\n",
    "                                       'Unique Count': unique_counts_non_numeric,\n",
    "                                       'Percentage Nan': percentage_nan_non_numeric})\n",
    "\n",
    "# print the non-numeric statistics dataframe\n",
    "print(tabulate(non_numeric_statistics, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "# Calculate shape\n",
    "print(\"There are {} rows and {} columns in the dataset\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for the 'sellingprice' column\n",
    "sellingprice_stats = df['sellingprice'].describe()\n",
    "\n",
    "# Convert the series to a DataFrame for tabulation\n",
    "sellingprice_stats_df = pd.DataFrame({'Selling Price Stats': sellingprice_stats})\n",
    "\n",
    "# Print the summary statistics using tabulate\n",
    "print(tabulate(sellingprice_stats_df, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Correlation and descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Spectral')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inncluding the categorical features to check the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df2 = df.copy()\n",
    "\n",
    "# Factorize categorical columns to convert categorical data into numerical format\n",
    "for column in df2.select_dtypes(exclude=[np.number]).columns:\n",
    "     df2[column], labels = pd.factorize(df[column])\n",
    "   \n",
    "# Compute the correlation matrix\n",
    "corr_matrix = df2.corr()\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.clustermap(corr_matrix, annot=True, cmap='Spectral', fmt=\".2f\", vmin=-0.5, vmax=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correaltion matrix above the following explanatory features are selected: \n",
    "year, condition, odometer, color and interior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the predicted column: selling_price using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['sellingprice'].quantile(0.25)\n",
    "Q3 = df['sellingprice'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['sellingprice'] < lower_bound) | (df['sellingprice'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['sellingprice'] >= lower_bound) & (df['sellingprice'] <= upper_bound) & (df['sellingprice'] != 1)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the odometer column using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['odometer'].quantile(0.25)\n",
    "Q3 = df['odometer'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['odometer'] < lower_bound) | (df['odometer'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['odometer'] >= lower_bound) & (df['odometer'] <= upper_bound)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are outliers in the year column using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "Q1 = df['year'].quantile(0.25)\n",
    "Q3 = df['year'].quantile(0.75)\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Print relevant values for debugging\n",
    "print(f'- Q1: {Q1} | Q3: {Q3} | IQR: {IQR}')\n",
    "print(f'- Lower Bound: {lower_bound} | Upper Bound: {upper_bound}')\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['year'] < lower_bound) | (df['year'] > upper_bound)]\n",
    "\n",
    "# Print the number of outliers\n",
    "num_outliers = len(outliers)\n",
    "print(f'- Number of outliers: {num_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows before removing outliers\n",
    "num_rows_before = len(df)\n",
    "\n",
    "# Remove outliers from the 'sellingprice' column including the sellingprice = 1 \n",
    "df = df[(df['year'] >= lower_bound) & (df['year'] <= upper_bound)]\n",
    "\n",
    "# Get the number of outliers removed\n",
    "num_outliers_removed = num_rows_before - len(df)\n",
    "print(f'Number of outliers removed: {num_outliers_removed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 'ID' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keeping the columns which are needed \n",
    "df = df[['year', 'condition', 'odometer', 'color', 'interior', 'sellingprice']]\n",
    "#Add an 'id' column with values starting from 1\n",
    "df['ID']=range(1, len(df) +1) \n",
    "\n",
    "#Moving the 'id' column to the front\n",
    "df.insert(0, 'ID', df.pop('ID'))\n",
    "\n",
    "#Display the updated datframe \n",
    "print(tabulate(df.head(), headers='keys', tablefmt='fancy_grid'))\n",
    "print(tabulate(df.tail(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the unique values of the explanatory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate({\n",
    "    'Year': df['year'].unique(),\n",
    "    'Condition': df['condition'].unique(),\n",
    "    'Odometer': df['odometer'].unique(),\n",
    "    'Color': df['color'].unique(),\n",
    "    'Interior': df['interior'].unique(),\n",
    " }, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts to identify the categories for the column \"colors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(df['color'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the categories for the column \"color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize colors\n",
    "def categorize_color(color):\n",
    "     \n",
    "    if 'black' in color:\n",
    "        return 'black'\n",
    "    elif 'white' in color:\n",
    "        return 'white'\n",
    "    elif 'silver' in color:\n",
    "        return 'silver'\n",
    "    elif 'gray' in color:\n",
    "        return 'gray'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Apply the function to create a new 'color_category' column\n",
    "df['color_cat'] = df['color'].apply(categorize_color)\n",
    "\n",
    "# Display the count of unique values in 'color_category'\n",
    "print(tabulate(df['color_cat'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts to identify the categories for the column \"colors\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(df['interior'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating categories for column 'interior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize interior\n",
    "def categorize_interior(interior):\n",
    "     \n",
    "    if 'black' in interior:\n",
    "        return 'black'\n",
    "    # elif 'beige' in interior:\n",
    "    #     return 'beige'\n",
    "    elif 'gray' in interior:\n",
    "        return 'gray'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Apply the function to create a new 'color_category' column\n",
    "df['interior_cat'] = df['interior'].apply(categorize_interior)\n",
    "\n",
    "# Display the count of unique values in 'color_category'\n",
    "print(tabulate(df['interior_cat'].value_counts().reset_index(), headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the color and interior column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['interior', 'color'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Distribution visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the current distribution of sellingprice after cleaning the data\n",
    "- And normalizing the data in this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply square root transformation to 'sellingprice'\n",
    "df['sqrt_sellingprice'] = np.sqrt(df['sellingprice'])\n",
    "\n",
    "# Set a smaller figure size\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Set font size for better readability\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "# Create a histogram of the 'sellingprice' column\n",
    "plt.subplot(1, 2, 1)\n",
    "histplot = sns.histplot(df['sellingprice'], bins=20, kde=True, color='slateblue')\n",
    "\n",
    "# Add frequency counts to each bin\n",
    "for rect in histplot.patches:\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "\n",
    "    # Add the text annotation\n",
    "    histplot.text(x + width / 2, y + height / 1.5 , f'{int(height)}', ha='center', va='center_baseline')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Distribution of Selling Prices')\n",
    "plt.xlabel('Selling Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create a histogram of the 'sellingprice' column\n",
    "plt.subplot(1, 2, 2)\n",
    "histplot = sns.histplot(df['sqrt_sellingprice'], bins=20, kde=True, color='slateblue')\n",
    "\n",
    "# Add frequency counts to each bin\n",
    "for rect in histplot.patches:\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x = rect.get_x()\n",
    "    y = rect.get_y()\n",
    "\n",
    "    # Add the text annotation\n",
    "    histplot.text(x + width / 2, y + height / 1.9 , f'{int(height)}', ha='center', va='center_baseline')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Distribution of Normalized Selling Prices')\n",
    "plt.xlabel('Selling Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definine the function generate_freq_table to visualize the categorical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the target variable and calculate the size of each group\n",
    "def generate_freq_table(df, variable = ['sellingprice']):\n",
    "    dfs = []\n",
    "    for i in variable:\n",
    "        df_count = (\n",
    "            df.groupby(i, observed=False)\n",
    "            .size()\n",
    "            .reset_index(name='N')\n",
    "            .assign(var = i)\n",
    "            .rename(columns={i: 'category'})\n",
    "        )\n",
    "        # Append the result to the list of DataFrames\n",
    "        dfs.append(df_count)\n",
    "        # Concatenate all DataFrames in the list\n",
    "        res = pd.concat(dfs)\n",
    "        # Convert the 'category' column to string type\n",
    "        res['category'] = res['category'].astype(str)\n",
    "    # Return the final concatenated DataFrame\n",
    "    return res\n",
    "\n",
    "generate_freq_table(df, ['interior_cat', 'color_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to create the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freq_plot(freq_table):\n",
    "    return (\n",
    "        ggplot(freq_table, aes(x='var', y='N', fill='category')) +\n",
    "        geom_col(stat='identity', position='dodge')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the interior categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_freq_plot(generate_freq_table(df, ['interior_cat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the color categories plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_freq_plot(generate_freq_table(df, ['color_cat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the plot for year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the frequency plot of year with different colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "histplot = sns.histplot(data=df, x='year', hue='year', bins=15, palette='viridis', kde=False)\n",
    "plt.title('Frequency Plot of Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the plot for condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the frequency plot of condition with different colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "histplot = sns.histplot(data=df, x='condition', hue='condition', bins=41, palette='magma')\n",
    "plt.title('Frequency Plot of Condition')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "#Modify legend \n",
    "plt.legend(title='Condition', loc='upper left', ncol=2, labels= (df['condition'].unique()))\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating the plot for odometer\n",
    "- And normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply square root transformation to 'age'\n",
    "df['sqrt_odometer'] = np.sqrt(df['odometer'])\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(16, 6))  # Increased the figure width to accommodate both plots\n",
    "\n",
    "# Create the first histogram of the 'odometer' column\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['odometer'], bins=20, color='lightseagreen')\n",
    "plt.title('Histogram of Odometer')\n",
    "plt.xlabel('Odometer')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create the second histogram of a different column (replace 'another_column' with the actual column name)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['sqrt_odometer'], bins=20, color='lightseagreen')  # Replace 'another_column' with the actual column name\n",
    "plt.title('Histogram of Normalized Odometer')\n",
    "plt.xlabel('Odometer logged')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Trainclasssplitter class with 3 def fucntions:\n",
    "- Innitialize instances\n",
    "- Statistics calculator\n",
    "- Train, test and validation splitter\n",
    "It is important to note that the splitter splits based on an even distribution of the sellingprice target variable for each of the sets. This is done by transforming it to a categorical variable for the time being by creating sellingprice bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestSplitter(object):\n",
    "    '''Class to perform the split of the data into train, test, and validation.'''\n",
    "    def __init__(self, train_frac=0.8, validation_frac=0.2, seed=1234, num_bins=5):\n",
    "        # Initialize the splitter with specified fractions, seed, and number of bins\n",
    "        self.train_frac = train_frac\n",
    "        self.validation_frac = validation_frac\n",
    "        self.seed = seed\n",
    "        self.num_bins = num_bins\n",
    "        self.total_n_sellingprice_bin = {}  # Dictionary to store total counts for each sellingprice bin\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        # Calculate statistics for each split (train, test, validation)\n",
    "        statistics = {}\n",
    "        for i in ['train_set', 'test_set', 'validation_set']:\n",
    "            split_stats = {}\n",
    "            \n",
    "            # Create bins for sellingprice in the current split\n",
    "            bins = np.linspace(getattr(self, i)['sellingprice'].min(), getattr(self, i)['sellingprice'].max(), self.num_bins + 1)\n",
    "            getattr(self, i)['sellingprice_bin'] = pd.cut(getattr(self, i)['sellingprice'], bins=bins)\n",
    "            \n",
    "            # Count the occurrences of each sellingprice bin in the current split\n",
    "            target_count = getattr(self, i).groupby('sellingprice_bin').size().reset_index()\n",
    "            \n",
    "            for _, row in target_count.iterrows():\n",
    "                bin_label = str(row['sellingprice_bin'])\n",
    "                bin_count = row[0]\n",
    "                percentage_key = f'percentage_total_sellingprice_bin_{bin_label}'\n",
    "                \n",
    "                # Store counts and percentages in the split_stats dictionary\n",
    "                split_stats[f'N_sellingprice_bin_{bin_label}'] = bin_count\n",
    "                split_stats[percentage_key] = bin_count / self.total_n_sellingprice_bin.get(bin_label, 1) * 100\n",
    "\n",
    "            statistics[i] = split_stats\n",
    "\n",
    "        self.split_statistics = statistics\n",
    "\n",
    "    def split_train_test(self, df):\n",
    "        print(\"Generating the train/validation/test splits...\")\n",
    "        \n",
    "        # Create bins for sellingprice in the entire dataset\n",
    "        bins = np.linspace(df['sellingprice'].min(), df['sellingprice'].max(), self.num_bins + 1)\n",
    "        df['sellingprice_bin'] = pd.cut(df['sellingprice'], bins=bins)\n",
    "        \n",
    "        # Count the total occurrences of each sellingprice bin in the entire dataset\n",
    "        for bin_label in df['sellingprice_bin'].unique():\n",
    "            bin_count = df.loc[lambda x: x.sellingprice_bin == bin_label].shape[0]\n",
    "            self.total_n_sellingprice_bin[str(bin_label)] = bin_count\n",
    "\n",
    "        # Sample the training set using the specified fraction and seed\n",
    "        self.train_set = df.sample(frac=self.train_frac, random_state=self.seed)\n",
    "        # Create the test set by excluding IDs present in the training set\n",
    "        self.test_set = df.loc[lambda x: ~x.ID.isin(self.train_set.ID)].reset_index(drop=True)\n",
    "        # Sample the validation set from the training set\n",
    "        self.validation_set = self.train_set.sample(frac=self.validation_frac).reset_index(drop=True)\n",
    "        # Exclude validation set IDs from the training set\n",
    "        self.train_set = self.train_set.loc[lambda x: ~x.ID.isin(self.validation_set.ID)].reset_index(drop=True)\n",
    "        \n",
    "        print(\"Calculating the statistics...\")\n",
    "        # Calculate and store statistics for the splits\n",
    "        self.calculate_statistics()\n",
    "        print(\"Split completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to perform the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fitting_splits object that will hold the train, validation, and test data\n",
    "fitting_splits = TrainTestSplitter()\n",
    "fitting_splits.split_train_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some statistics of the split. The goal is to have an even percentage of the data per bin. These percentages should be similar per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_splits.test_set.shape\n",
    "print(tabulate(fitting_splits.split_statistics, headers=\"keys\", tablefmt= \"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create 3 def functions:\n",
    "- Dummificator\n",
    "- Scaler\n",
    "- A function that uses the other 2 def functions\n",
    "\n",
    "The code takes onehot encoded features into consideration and makes sure that these columns are not used for the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def fucntion for dummification\n",
    "def dummify(df, one_hot_encoder):\n",
    "    # Specify the variables to encode\n",
    "    vars_to_encode = ['interior_cat', 'color_cat']\n",
    "    \n",
    "    # Extract the columns to be encoded\n",
    "    df_to_encode = df[vars_to_encode]\n",
    "    \n",
    "    # Initialize or use the provided OneHotEncoder\n",
    "    if not one_hot_encoder:\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        df_encoded = one_hot_encoder.fit_transform(df_to_encode).toarray()\n",
    "    else:\n",
    "        df_encoded = one_hot_encoder.transform(df_to_encode).toarray()\n",
    "    \n",
    "    # Create a DataFrame from the encoded columns\n",
    "    df_encoded = pd.DataFrame(df_encoded, columns=one_hot_encoder.get_feature_names_out())\n",
    "    \n",
    "    # Add the encoded columns and drop the original columns\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "    df = df.drop(vars_to_encode, axis=1)\n",
    "    \n",
    "    return df, one_hot_encoder\n",
    "\n",
    "# Def fucntion for standard scaling\n",
    "def scale(df, standard_scaler, cols_to_scale):\n",
    "    # Initialize or use the provided StandardScaler\n",
    "    if not standard_scaler:\n",
    "        standard_scaler = StandardScaler()\n",
    "        df[cols_to_scale] = standard_scaler.fit_transform(df[cols_to_scale])\n",
    "    else:\n",
    "        df[cols_to_scale] = standard_scaler.transform(df[cols_to_scale])\n",
    "    \n",
    "    return df, standard_scaler\n",
    "\n",
    "# Def function for data preperation\n",
    "def prepare_data(df, one_hot_encoder=None, standard_scaler=None, cols_to_scale=None):\n",
    "    # Reset the index of the DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    df, one_hot_encoder = dummify(df, one_hot_encoder)\n",
    "    \n",
    "    # Identify columns to scale (numerical features)\n",
    "    if cols_to_scale is None:\n",
    "        cols_to_scale = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Identify columns to exclude from scaling (one-hot encoded columns)\n",
    "    one_hot_columns = one_hot_encoder.get_feature_names_out() if one_hot_encoder else []\n",
    "    cols_to_exclude = df.columns[df.columns.isin(one_hot_columns)]\n",
    "    \n",
    "    # Remove one-hot encoded columns from the list of columns to scale\n",
    "    cols_to_scale = list(set(cols_to_scale) - set(cols_to_exclude))\n",
    "    \n",
    "    # Perform feature scaling\n",
    "    df, standard_scaler = scale(df, standard_scaler, cols_to_scale)\n",
    "    \n",
    "    return df, one_hot_encoder, standard_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inhere, 8 datasets are made:\n",
    "- X_train \n",
    "- y_train\n",
    "- x_validation\n",
    "- y_validation\n",
    "- x_test\n",
    "- y_test\n",
    "- x_train_validation\n",
    "- y_train_validation\n",
    "\n",
    "The dataset 'x_train_validation' and 'y_train_validation' are concatenations of the train and validation sets.\n",
    "\n",
    "For every dataset, the onehotencoder, scaler and required features are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all the data for subsequent use\n",
    "# Training Set\n",
    "X_train, one_hot_encoder, standard_scaler = prepare_data(fitting_splits.train_set)\n",
    "\n",
    "# Drop unnecessary columns from the training set\n",
    "X_train = X_train.drop([\"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice', 'sellingprice_bin'], axis=1)\n",
    "\n",
    "# Extract the target variable for the training set (square root of selling price)\n",
    "y_train = fitting_splits.train_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "# Validation Set\n",
    "# Use the same one-hot encoder and standard scaler as the training set\n",
    "X_validation = prepare_data(fitting_splits.validation_set, one_hot_encoder, standard_scaler)[0]\n",
    "\n",
    "# Drop unnecessary columns from the validation set\n",
    "X_validation = X_validation.drop([\"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice', 'sellingprice_bin'], axis=1)\n",
    "\n",
    "# Extract the target variable for the validation set (square root of selling price)\n",
    "y_validation = fitting_splits.validation_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "# Test Set\n",
    "# Use the same one-hot encoder and standard scaler as the training set\n",
    "X_test = prepare_data(fitting_splits.test_set, one_hot_encoder, standard_scaler)[0]\n",
    "\n",
    "# Drop unnecessary columns from the test set\n",
    "X_test = X_test.drop(['sellingprice_bin', \"ID\", 'odometer', 'sellingprice', 'sqrt_sellingprice'], axis=1)\n",
    "\n",
    "# Extract the target variable for the test set (square root of selling price)\n",
    "y_test = fitting_splits.test_set[\"sqrt_sellingprice\"]\n",
    "\n",
    "# Combine Training and Validation Sets for Cross-Validation\n",
    "X_train_validation = pd.concat([X_train, X_validation])\n",
    "y_train_validation = pd.concat([y_train, y_validation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fitting: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fitted trying to find the best model in a range of hyperparameters. The best model is based on the lowest MSE score, instead of the f1 score for classification problems\n",
    "\n",
    "It uses 5-fold cross-validation to find the best hyperparameters by doing the k-fold cross-validation calculations without using sklearn liberaries. \n",
    "\n",
    "A pickle is saved to avoid resoing the grid search process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pickle has been made of the outcomes of the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the pickle file already exists\n",
    "pickle_filename = 'grid_search_rf_model.pkl'\n",
    "if os.path.exists(pickle_filename):\n",
    "    # Load the existing GridSearchCV object\n",
    "    with open(pickle_filename, 'rb') as file:\n",
    "        sklearn_grid_search_rf = pickle.load(file)\n",
    "else:\n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [x for x in range(10, 200, 10)],\n",
    "        \"max_depth\": [x for x in range(5, 21, 5)]\n",
    "    }\n",
    "\n",
    "    # Create a RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor()\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    sklearn_grid_search_rf = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "\n",
    "    # Fit the GridSearchCV object to your data\n",
    "    sklearn_grid_search_rf.fit(X_train_validation, y_train_validation)\n",
    "\n",
    "    # Save the GridSearchCV object\n",
    "    with open(pickle_filename, 'wb') as file:\n",
    "        pickle.dump(sklearn_grid_search_rf, file)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    mse_scores = []\n",
    "\n",
    "    # Define the number of folds for cross-validation\n",
    "    num_folds = 5\n",
    "\n",
    "    # Get the number of samples in your dataset\n",
    "    num_samples = X_train_validation.shape[0]\n",
    "\n",
    "    # Calculate the size of each fold\n",
    "    fold_size = num_samples // num_folds\n",
    "\n",
    "    # Iterate over the folds\n",
    "    for fold in range(num_folds):\n",
    "        # Define the indices for the current fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < num_folds - 1 else num_samples\n",
    "        \n",
    "        # Create training and validation sets for the current fold\n",
    "        X_train_fold = np.concatenate([X_train_validation[:start_idx], X_train_validation[end_idx:]])\n",
    "        y_train_fold = np.concatenate([y_train_validation[:start_idx], y_train_validation[end_idx:]])\n",
    "        \n",
    "        X_val_fold = X_train_validation[start_idx:end_idx]\n",
    "        y_val_fold = y_train_validation[start_idx:end_idx]\n",
    "\n",
    "        # Create a RandomForestRegressor\n",
    "        rf_model = RandomForestRegressor()\n",
    "\n",
    "        # Set up the model with specified parameters\n",
    "        rf_model.set_params(**sklearn_grid_search_rf.best_params_)\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        rf_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions on the validation fold\n",
    "        y_pred = rf_model.predict(X_val_fold)\n",
    "\n",
    "        # Calculate mean squared error for the fold\n",
    "        mse_fold = np.mean((y_val_fold - y_pred) ** 2)\n",
    "\n",
    "        # Append the MSE for the fold to the list\n",
    "        mse_scores.append(mse_fold)\n",
    "\n",
    "    # Calculate the average MSE across all folds\n",
    "    average_mse = np.mean(mse_scores)\n",
    "\n",
    "    print(\"Average Mean Squared Error (MSE) across 5 folds:\", average_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average MSE across each of the folds is 459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model evaluation: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model can not be done based of the f1 score since it is a regression problem and not a classification problem. Therefore, the MSE is used as metric of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters found by the grid search\n",
    "print(\"Best Parameters: \", sklearn_grid_search_rf.best_params_)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_model = sklearn_grid_search_rf.best_estimator_\n",
    "\n",
    "# Print the best cross-validated score (negative mean squared error) obtained during the grid search\n",
    "print(\"Best Cross-Validated Score (MSE): \", -sklearn_grid_search_rf.best_score_)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error on the test set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the mean squared error on the test set\n",
    "print(\"Test Set MSE: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is as good as equal to the cross validation MSE.\n",
    "\n",
    "We can conclude that the model shows:\n",
    "- Consistent Performance:\n",
    "\n",
    "If the test set MSE is close to the cross-validated MSE, it could indicate that your model is generalizing well to unseen data. This consistency suggests that the performance observed during cross-validation is a good reflection of how the model will perform on new, unseen samples.\n",
    "\n",
    "- Appropriate Model Complexity:\n",
    "\n",
    "The fact that the best hyperparameters from the cross-validation also perform well on the test set suggests that the chosen model complexity (determined by hyperparameters like max_depth and n_estimators in the case of a RandomForestRegressor) is appropriate for the problem at hand.\n",
    "\n",
    "- Reliable Evaluation:\n",
    "\n",
    "The cross-validated MSE serves as an estimate of the model's expected performance on new data. When the test set performance aligns with the cross-validated performance, it adds confidence to the reliability of the cross-validated estimate.\n",
    "\n",
    "- No Overfitting:\n",
    "\n",
    "If the test set MSE was significantly higher than the cross-validated MSE, it might indicate overfitting, where the model performs well on the training data but poorly on new data. The fact that they are comparable suggests that overfitting is less of a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two plots to visualize our model's performance. The first plot compares the actual values with the predicted values. The second plot compares the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals by subtracting predicted values from actual values\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Scatter Plot: Visualizing predicted vs. actual selling prices with residuals as color\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Predicted Selling Prices\")\n",
    "plt.title(\"Actual vs. Predicted Selling Prices\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Residual Plot: Visualizing residuals against actual selling prices\n",
    "plt.figure()\n",
    "plt.scatter(y_test, residuals, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our Test Set MSE is 459, but what does this mean? A good way to evaluate this is by comparing it to the variance of our target variable sellingprice. Please keep in mind that this is the squareroot version of sellingprice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the Variance of the Target Variable\n",
    "variance = np.var(y_test)\n",
    "\n",
    "# Calculate R-squared, a measure of how well the model explains the variance in the target variable\n",
    "r_squared_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the Ratio of MSE to Variance, a metric to assess model performance\n",
    "mse_to_variance_ratio_rf = mse_rf / variance\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f'Mean Squared Error (MSE): {mse_rf}')\n",
    "print(f'Variance of the Target Variable: {variance}')\n",
    "print(f'R-squared: {r_squared_rf}')\n",
    "print(f'MSE to Variance Ratio: {mse_to_variance_ratio_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the outcomes\n",
    "\n",
    "- Mean Squared Error (MSE): 459\n",
    "\n",
    "An MSE of 459.10 indicates that, on average, the squared difference between your predicted and actual values is relatively moderate. The mean error is around 21 dollar, but since this is based on squareroot sellingprice, we have to square it again. So the average discrepancy of a prediction is 459 dollars, which is a reasonable outcome\n",
    "\n",
    "- Variance of the Target Variable: 1213.13\n",
    "\n",
    "In comparison to the variance, the MSE is smaller, suggesting that the model is capturing a significant portion of the variability.\n",
    "\n",
    "- R-squared: 0.62\n",
    "\n",
    "The R-squared value of 0.62 indicates that the model explains approximately 62% of the variability in the target variable. This is a moderate level of explanatory power.\n",
    "\n",
    "- MSE to Variance Ratio: 0.38\n",
    "\n",
    "The MSE to Variance Ratio is less than 1, which suggests that the model is performing reasonably well. A ratio closer to 0 indicates that the model is capturing a substantial portion of the variance relative to the inherent variability in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Fitting: ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to know the amount of neurons in the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of neurons for the input layer must be: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates the model\n",
    "# note that we pass the number of neurons as a parameter to the network\n",
    "def create_model(neurons=10):\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(neurons, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "    nn_model.add(Dense(1))\n",
    "    nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return nn_model\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "# turn the keras model into a sklearn compatible model\n",
    "# note that the neurons parameter needs to be specified in the interface of KerasClassifier\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the hyperparameters that can be used for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "batch_size = [10, 20]\n",
    "epochs = [100, 150]\n",
    "neurons = [10, 20, 30]\n",
    "params_grid = dict(batch_size=batch_size, epochs=epochs, neurons=neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform grid search for hyperparameter tuning and save it in a pickle, if the pickle already exists this step is not repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pickle file path\n",
    "pickle_file_path = 'nn_grid_search.pkl'\n",
    "\n",
    "# perform grid search with sklearn if needed, otherwise load the grid search already performed\n",
    "if os.path.exists(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as handle:\n",
    "        grid_search_nn = pickle.load(handle)\n",
    "else:\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    def create_model(neurons=10):\n",
    "        nn_model = Sequential()\n",
    "        nn_model.add(Dense(neurons, input_dim=X_train_validation.shape[1], activation=\"relu\"))\n",
    "        nn_model.add(Dense(1))\n",
    "        nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        return nn_model\n",
    "\n",
    "    model = KerasRegressor(neurons=10, build_fn=create_model, verbose=0)\n",
    "    \n",
    "    # Create GridSearchCV with the modified model and parameter grid\n",
    "    grid_search_nn = GridSearchCV(estimator=model, param_grid=params_grid, n_jobs=-1, cv=5)\n",
    "    grid_search_nn = grid_search_nn.fit(X_train_validation, y_train_validation)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    print(f'hypertuning with sklearn grid search for neural networks complete in {round((end_time - start_time).seconds/60, 2)} minutes')\n",
    "    \n",
    "    # store the results of the grid search to disk\n",
    "    with open(pickle_file_path, 'wb') as handle:\n",
    "        pickle.dump(grid_search_nn, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation: ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or return the results\n",
    "print(\"Best Parameters:\", grid_search_nn.best_params_)\n",
    "print(\"Best Cross-Validated Score (MSE):\", -grid_search_nn.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating test set MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best model\n",
    "best_model = grid_search_nn.best_estimator_\n",
    "\n",
    "# Predict y based on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the MSE\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test Set MSE:\", mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results for each parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = grid_search_nn.cv_results_\n",
    "print(\"Grid Search Results:\")\n",
    "for params, mean_score, std_score in zip(all_results['params'], all_results['mean_test_score'], all_results['std_test_score']):\n",
    "    print(f\"Parameters: {params}, Mean Score (MSE): {-mean_score}, Standard Deviation: {std_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the MSE to the variance again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_ann = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the Variance of the Target Variable\n",
    "variance = np.var(y_test)\n",
    "\n",
    "# Calculate R-squared, a measure of how well the model explains the variance in the target variable\n",
    "r_squared_ann = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the Ratio of MSE to Variance, a metric to assess model performance\n",
    "mse_to_variance_ratio_ann = mse_ann / variance\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f'Mean Squared Error (MSE): {mse_ann}')\n",
    "print(f'Variance of the Target Variable: {variance}')\n",
    "print(f'R-squared: {r_squared_ann}')\n",
    "print(f'MSE to Variance Ratio: {mse_to_variance_ratio_ann}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean Squared Error (MSE): 460.49\n",
    "\n",
    "This 460.49 suggests a moderate level of prediction error. The interpretation of MSE depends on the scale of your target variable.\n",
    "\n",
    "- Variance of the Target Variable: 1213.13\n",
    "\n",
    "- R-squared: 0.6204\n",
    "\n",
    "R-squared of 0.62 indicates that your model explains approximately 62% of the variability in the target variable.\n",
    "\n",
    "- MSE to Variance Ratio: 0.3796\n",
    "\n",
    "The ratio is 0.38, suggesting that the prediction error is relatively lower compared to the variance of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals by subtracting predicted values from actual values\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Scatter Plot: Visualizing predicted vs. actual selling prices with residuals as color\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Predicted Selling Prices\")\n",
    "plt.title(\"Actual vs. Predicted Selling Prices\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Residual Plot: Visualizing residuals against actual selling prices\n",
    "plt.figure()\n",
    "plt.scatter(y_test, residuals, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model fitting: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit and saved in a pickle, unless the pickle already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pickle file path\n",
    "pickle_file_path = 'grid_search_xgb_model.pkl'\n",
    "\n",
    "# Check if the pickle file already exists\n",
    "if os.path.exists(pickle_file_path):\n",
    "    # Load the existing GridSearchCV object\n",
    "    with open(pickle_file_path, 'rb') as file:\n",
    "        grid_search = pickle.load(file)\n",
    "else:\n",
    "    # Create an XGBoost regressor\n",
    "    xg_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "    # Define a parameter grid to search\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Save the GridSearchCV object (including best model) to a pickle file\n",
    "    with open(pickle_file_path, 'wb') as file:\n",
    "        pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model evaluation: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or return the results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated Score (MSE):\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testset MSE is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test Set MSE:\", mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resuts are again compared with the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the Variance of the Target Variable\n",
    "variance = np.var(y_test)\n",
    "\n",
    "# Calculate R-squared, a measure of how well the model explains the variance in the target variable\n",
    "r_squared_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the Ratio of MSE to Variance, a metric to assess model performance\n",
    "mse_to_variance_ratio_xgb = mse_xgb / variance\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f'Mean Squared Error (MSE): {mse_xgb}')\n",
    "print(f'Variance of the Target Variable: {variance}')\n",
    "print(f'R-squared: {r_squared_xgb}')\n",
    "print(f'MSE to Variance Ratio: {mse_to_variance_ratio_xgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again two plots are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals by subtracting predicted values from actual values\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Scatter Plot: Visualizing predicted vs. actual selling prices with residuals as color\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Predicted Selling Prices\")\n",
    "plt.title(\"Actual vs. Predicted Selling Prices\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Residual Plot: Visualizing residuals against actual selling prices\n",
    "plt.figure()\n",
    "plt.scatter(y_test, residuals, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model fitting: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model is fitted and again saved in a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the pickle file\n",
    "pickle_file_path = 'grid_search_knn_model.pkl'\n",
    "\n",
    "# Check if the pickle file already exists\n",
    "if os.path.exists(pickle_file_path):\n",
    "    # Load the GridSearchCV object from the pickle file\n",
    "    with open(pickle_file_path, 'rb') as file:\n",
    "        grid_search = pickle.load(file)\n",
    "else:\n",
    "    # Create a KNeighborsRegressor\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "\n",
    "    # Define a parameter grid to search\n",
    "    param_grid = {\n",
    "        'n_neighbors': [x for x in range(1, 40)],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "        'metric': ['minkowski', 'cosine']\n",
    "    }\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=knn_reg, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Save the GridSearchCV object (including the best model) to a pickle file\n",
    "    with open(pickle_file_path, 'wb') as file:\n",
    "        pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model evaluation: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the best parameters and the MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or return the results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validated Score (MSE):\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the test set MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict y\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate MSE of test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test Set MSE:\", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_knn = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the Variance of the Target Variable\n",
    "variance = np.var(y_test)\n",
    "\n",
    "# Calculate R-squared, a measure of how well the model explains the variance in the target variable\n",
    "r_squared_knn = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the Ratio of MSE to Variance, a metric to assess model performance\n",
    "mse_to_variance_ratio_knn = mse_knn / variance\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f'Mean Squared Error (MSE): {mse_knn}')\n",
    "print(f'Variance of the Target Variable: {variance}')\n",
    "print(f'R-squared: {r_squared_knn}')\n",
    "print(f'MSE to Variance Ratio: {mse_to_variance_ratio_knn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals by subtracting predicted values from actual values\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Scatter Plot: Visualizing predicted vs. actual selling prices with residuals as color\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Predicted Selling Prices\")\n",
    "plt.title(\"Actual vs. Predicted Selling Prices\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Residual Plot: Visualizing residuals against actual selling prices\n",
    "plt.figure()\n",
    "plt.scatter(y_test, residuals, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model fitting: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "linear_reg_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model evaluation: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MSE and R2-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate the Variance of the Target Variable\n",
    "variance = np.var(y_test)\n",
    "\n",
    "# Calculate R-squared, a measure of how well the model explains the variance in the target variable\n",
    "r_squared_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate the Ratio of MSE to Variance, a metric to assess model performance\n",
    "mse_to_variance_ratio_lr = mse_lr / variance\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f'Mean Squared Error (MSE): {mse_lr}')\n",
    "print(f'Variance of the Target Variable: {variance}')\n",
    "print(f'R-squared: {r_squared_lr}')\n",
    "print(f'MSE to Variance Ratio: {mse_to_variance_ratio_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals by subtracting predicted values from actual values\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Scatter Plot: Visualizing predicted vs. actual selling prices with residuals as color\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Predicted Selling Prices\")\n",
    "plt.title(\"Actual vs. Predicted Selling Prices\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Residual Plot: Visualizing residuals against actual selling prices\n",
    "plt.figure()\n",
    "plt.scatter(y_test, residuals, alpha=0.5, s=10, c=residuals, cmap='coolwarm')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0 for reference\n",
    "plt.xlabel(\"Actual Selling Prices\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "\n",
    "# Add a colorbar to indicate the magnitude of residuals\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Residuals')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table of all models results ranked on their MSE to variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for Random Forest\n",
    "results_rf = [\"Random Forest\", mse_rf, variance, r_squared_rf, mse_to_variance_ratio_rf]\n",
    "\n",
    "# Results for ANN\n",
    "results_ann = [\"Artificial Neural Network\", mse_ann, variance, r_squared_ann, mse_to_variance_ratio_ann]\n",
    "\n",
    "# Results for XGBoost\n",
    "results_xgb = [\"XGBoost\", mse_xgb, variance, r_squared_xgb, mse_to_variance_ratio_xgb]\n",
    "\n",
    "# Results for Linear Regression\n",
    "results_lr = [\"Linear Regression\", mse_lr, variance, r_squared_lr, mse_to_variance_ratio_lr]\n",
    "\n",
    "# Results for KNN\n",
    "results_knn = [\"K-Nearest Neighbors\", mse_knn, variance, r_squared_knn, mse_to_variance_ratio_knn]\n",
    "\n",
    "# Create a list of results\n",
    "all_results = [results_rf, results_ann, results_xgb, results_lr, results_knn]\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Model\", \"Mean Squared Error (MSE)\", \"Variance\", \"R-squared\", \"MSE to Variance Ratio\"]\n",
    "\n",
    "# Sort the results based on MSE to Variance Ratio\n",
    "all_results_sorted = sorted(all_results, key=lambda x: x[-1])  # Assuming the MSE to Variance Ratio is in the last column\n",
    "\n",
    "# Create the table\n",
    "table = tabulate(all_results_sorted, headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all_results_sorted from the previous example\n",
    "models = [result[0] for result in all_results_sorted]\n",
    "mse_to_variance_ratio = [result[-1] for result in all_results_sorted]\n",
    "\n",
    "# Bar Chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=models, y=mse_to_variance_ratio, palette='viridis', width=0.3)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MSE to Variance Ratio')\n",
    "plt.title('Model Comparison based on MSE to Variance Ratio')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0.35, max(mse_to_variance_ratio) + 0.01)  # Set y-axis limit\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('teaching')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1e146a432e2971ead6ca2adfccd361e7256016801521af6894ffdef8a064a536"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
